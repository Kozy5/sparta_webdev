
메모리 기본 내용중
조리대에 해당하는 캐시의
 **“지역성”, “캐시히트/미스”** 
 대해 학습 
 ** 메모리가 어떻게 사용되는지 
메모리 할당 대해 학습

 요것도, CS면접 단골 질문이라구요! ㅎㅎ

메모리심화

1-1. 캐시란
# 데이터를 미리 복사해 놓는 임시 저장소


우리가 보는 화면에 출력되는 데이터 
= 메인 메모리에 저장된 데이터

1. 프로그램 실행 -> 디스크를 읽어서 메인 메모리에 복사->
2. CPU(MMU)가 메인 메모리에서 데이터 읽어오며 작업 처리
3. 이때 캐시가 중간에서 한번더 메인메모리의 데이터 복사해둠

빠른 장치, 느린 장치 속도 차이로 인한 병목 현상 줄이기 위한 메모리!
데이터 접근에 오래 걸리는 경우 해결, 다시 계산하는 시간 절약
즉, 계층과 계층 사이에서 속도차이 해결하는 임시 저장소

레지스터 : 메모리, CPU 속도 차이 해결하기 위한 캐시
주기억장치 : 캐시 메모리, 보조기억장치 사이 속도 차이 해결하기 위한 캐시


SRAM(L1~3캐시)만  캐시라고 불리는 이유

● 레지스터는 CPU의 연산을 위한 저장소
● 메인 메모리 = 실제 프로그램 실행위한 저장소
● 그 사이에 정말 임시 저장소 역할만 하는 것이 SRAM이라서!

SRAM(L1~3캐시)
● 정적 메모리로써 용량 작음
● 속도 매우 빠름 -> 캐시 전용 메모리로 많이 쓰임


지역성의 원리

지역성 이란?

자주 사용되는 데이터 특성 의미
캐시를 직접 설정할때 = 자주 사용되는 데이터 기반으로 설정해야함
(이러한 특성을 지역성이라고 함)

1. 시간 지역성
● for문 속 선언된 i = 반복문 속 계속 접근 이루어지는 변수
● 최근에 사용했기 때문에 계속 접근해서 +1 이루어짐
(i 변수에 대한 시간 지역성)

for(let i=0; i<5; i++){
	console.log(i) // 0 1 2 3 4
}


2. 공간 지역성

● 최근 접근 데이터 이루고 있는 공간, 그 가까운 공간에 접근하는 특성

● 배열 arr이라는 공간에 i가 연속적으로 할당되어 접근하는 방식 
(arr 배열 원소에 대한 공간 지역성)

let arr = [];

for(let i=0; i<5; i++){
	arr.push(i)
}
// arr = [0,1,2,3,4]

1-2. 캐시히트와 캐시미스

캐시히트

● 캐시에서 원하는 데이터를 찾은 것

● 위치 가깝고, CPU 내부버스 기반 작동 = 빠르다
● 캐시히트 시 -> 해당 데이터를 제어장치를 거쳐 가져옴

캐시미스

● 해당 데이터가 캐시에 없다면 주메모리로 가서 데이터를 찾아오는 것

●메모리를 가져올때 시스템 버스 기반 작동 =  느리다


캐시가 히트되기 위해 매핑되는 방법

● CPU의 레지스터와 주 메모리(RAM)간에 데이터 주고 받을 때를 기반
주 메모리에 비해 굉장히 작은 레지스터가 캐시 계층으로써 역할 
-> 매핑이 중요

직접 매핑

● 메모리가 1~100이 있고 캐시가 1~10이 있다면 
-> 1:1~10, 2:11~20... 와 같이 매핑
● 빠른처리, 잦은 충돌 발생

연관 매핑

● 순서 일치X, 관련 있는 캐시, 메모리 매핑
● 충돌이 적음, 모든 블록을 탐색함 = 느린 처리

집합 연관 매핑

● 직접 매핑과 연관 매핑 합쳐 놓은 것
● 순서는 일치, 집합을 둬서 저장, 블록화되어 있음 = 검색 효율적



1-3. 메모리 할당

메모리에 프로그램을 할당할 때는 
시작 메모리 위치, 메모리의 할당 크기를 기반으로 할당

연속 할당과 불연속 할당으로 나뉨

연속할당

메모리에 '연속적으로' 공간을 할당하는 것
고정 분할 방식과 가변 분할 방식으로 나뉨

고정 분할 방식

● 메모리를 미리 나누어 관리하는 방식
● 한계
    ○ 내부 단편화 발생 ⭕

가변 분할 방식

● 매 시점 프로그램의 크기에 맞게 동적으로 
메모리를 나눠 사용하는 방식
● 종류
    1. 최초적합: 위에서부터 바로 보이는 공간에 바로 할당
    2. 최적적합: 가장 크기에 맞는 공간부터 채우고 나머지 할당
    3. 최악적합: 가장 크기가 큰 공간부터 채우고 나머지 할당
- 한계
● 내부 단편화 발생 ❌
● 외부 단편화 발생 ⭕

WHY?

1. 최초적합(First-fit)

● 메모리 내, 빈 공간 중 가장 먼저 발견되는 
첫 번째 공간에 블록을 할당.

● 블록 크기와 공간 크기가 불일치 
-> 남은 공간을 새로운 빈 공간으로 남김

● 초기 메모리 관리를 위한 간단한 방법
외부 조각화가 발생할 수 있음


2. 최적적합(Best-fit)

● 메모리 내, 블록 크기와 가장 근접한 크기를 가진 
가장 작은 빈 공간을 찾아 할당

● 블록 크기와 공간 크기가 불일치
-> 남은 공간을 새로운 빈 공간으로 남김

● 내부 조각화는 줄일 수 있음 But 메모리 탐색 시간 걸림
= 성능 저하가 있을 수 있다.


3. 최악적합(Worst-fit)

● 메모리 내, 가장 큰 빈 공간을 찾아 블록을 할당

● 할당된 블록 크기와 공간 크기가 불일치 
-> 외부 조각화가 발생합니다.

● 내부 조각화 적음 But 빈 공간 찾는 데 시간이 걸림
= 성능 저하가 있을 수 있다.
         


# 내부단편화 & 외부단편화
            
 내부 단편화
            
 ●메모리를 나눈 크기보다 프로그램이 작아서 
들어가지 못하는 공간이 많이 발생하는 현상
즉, 들어갈 수 있는 공간보다 프로그램이 작아서 공간이 남아버리는 것
            
ex) 나눈 크기는 10씩. 10이라는 메모리공간에 
8크기의 프로그램이 할당 -> 2라는 메모리 공간이 남는다.
 
외부 단편화
● 메모리를 나눈 크기보다 프로그램이 커서 
들어가지 못하는 공간이 많이 발생하는 현상
즉, 들어갈 공간보다 들어갈 것이 더 커서 
들어가지 못하고 남아버리는 것



# 불연속 할당

운영체제에서는 여러개의 작업을 효율적으로 
수행해야하기 때문에 불연속 할당방법을 사용함


불연속 할당 방식의 단점

메모리 공간 할당, 해제 시 오버헤드
발생가능성 (불필요 할당)
메모리 공간이 분산 
-> 프로세스가 불연속 공간에 할당될 경우 
프로세스의 페이지 교체와 같은 작업이 더 복잡 
(교체 알고리즘 최적화 필요)


운영체제에서 불연속 할당을 사용하는 3가지 방법

1. 링크드 리스트(Linked List) 
: 불연속 공간에 프로세스를 할당할 때, 
할당된 공간의 주소를 연결리스트에 저장하는 방식
메모리 할당, 해제가 빠름 But 
공간 낭비 발생 가능

2. 비트맵(Bitmap) 
: 메모리 공간의 각 블록을 0 또는 1로 표시하여 
사용 가능한 블록과 사용 중인 블록을 구분하는 방식 
링크드 리스트보다 효율적인 공간 관리 제공 But
메모리 크기 크면 -> 비트맵이 매우 커지는 단점


3. 페이지 테이블(Page Table) 
: 가상 메모리 시스템에서 사용되는 방식
물리적인 주소 공간 = 페이지(작은 블록) 나누어 사용
각 프로세스는 자신의 페이지 테이블을 가지며
페이지 테이블 = 물리적인 주소, 가상 주소 매핑하는 역할만
링크드 리스트와 비트맵보다 효율적!
가상 메모리 구현하는 데 필요한 기술

메모리를 동일한 크기의 페이지로 나누고
프로그램마다 페이지 테이블을 두어 
이를 통해 
메모리에 프로그램을 할당


1) 페이징,
2) 세그멘테이션, 
3) 페이지드 세그멘테이션 
3가지 기법 존재

 1) 페이징

동일한 크기의 페이지 단위 나누어 메모리의 
서로 다른 위치에 프로세스를 할당
빈데이터(홀)의 크기가 균일하지 않은 문제가 해결
But 주소 변환 복잡

 2) 세그멘테이션

의미 단위인 세그먼트로 나누는 방식
코드와 데이터 기반, 함수 단위나눌 수 있음 
공유, 보안 GOOD
But 빈데이터(홀) 크기가 균일하지 않는 문제 발생

 3) 페이지드 세그멘테이션

공유, 보안 = 세그먼트
물리적 메모리 = 페이지

이렇게 나누는 방식


# 페이지 교체 알고리즘

- 메모리 내에 저장된 페이지 중에서 
어떤 페이지를 교체할지 결정하는 알고리즘 입니다.
- 페이지 교체 알고리즘은 
물리적인 메모리 공간이 한정되어 있을 때 
페이지 부재(page fault)가 발생하는 상황에서 
새로운 페이지를 적재하기 위해 
기존 페이지 중 어떤 페이지를 제거할지 
결정하는 역할을 합니다.

페이지 교체 알고리즘의 종류

종류               알고리즘    특징
간단한 알고리즘    무작위      무작위로 대상 페이지를 선정하여 스왑영역으로 보낸다.
                   FIFO        처음 메모리에 올라온 페이지를 스왑 영역으로 보낸다.

이론적 알고리즘    최적        미래의 접근 패턴을 보고 대상 페이지를 선정하여 스왑 영역으로 보낸다.

최적 근접 알고리즘 LRU         시간적으로 멀리 떨어진 페이지를 스왑 영역으로 보낸다.
                  LFU          사용 빈도가 적은 페이지를 스왑 영역으로 보낸다.
                  NUR          최근에 사용한 적이 없는 페이지를 스왑 영역으로 보낸다.
                  FIFO 변형    FIFO 알고리즘을 변형하여 성능을 높인다.



각 알고리즘은 다양한 방식으로 페이지를 교체, 
알고리즘에 따라 
페이지 부재율(page fault rate),
페이지 교체 오버헤드(page replacement overhead) 
등의 성능이 달라짐

1. 오프라인 알고리즘

● 먼미래에 참조되는 페이지와 현재 할당하는 페이지를 바꾸는 알고리즘
● 입력 데이터가 모두 주어진 상태에서 실행되는 알고리즘
 입력 데이터를 모두 가지고 있어서 실행 중에 새로운 입력 들어 오지 않음
● 입력 데이터를 한꺼번에 처리 가능,
 실행 시간, 공간 사용량 예측 가능 -> 입력 데이터 크기에 따라 
 적절한 실행 시간, 공간을 예약하여 처리 가능
● But, 미래에 사용되는 프로세스를 알 수 없음
● 메모리 할당에서는 사용할 수 없는 알고리즘, 
다른 알고리즘과의 성능 비교에 대한 기준을 제공


오프라인 알고리즘의 대표적인 예 = 정렬 알고리즘

정렬 알고리즘은 입력 데이터를 모두 가지고 있기에
 -> 오프라인 알고리즘이라고 볼 수 있다. 
 입력 데이터가 모두 주어졌으므로, 
한 번에 정렬 가능

1. FIFO (First In First Out) 선입 선출

- 가장 먼저 온 페이지를 교체 영역에 가장 먼저 놓는 방법입니다.
- 캐시 메모리에 새로운 데이터가 들어오면 가장 오래전에 들어온 데이터를 제거하고 새로운 데이터를 추가합니다.
- 이 방식은 구현이 간단하지만, 오래된 데이터가 최근에 사용된 데이터와 비슷한 경우에 성능이 저하될 수 있습니다.

2. LRU(Least Recently Used) 캐시히트하면 최신으로 갱신 / 교체 조건 -> 오래된거

- 참조가 가장 오래된 페이지를 교체하는 방법입니다.
- LRU 방식은 가장 최근에 사용된 데이터를 먼저 사용할 가능성이 높기 때문에 캐시 히트(hit)율을 높일 수 있습니다.
- 그러나 LRU 알고리즘의 구현 방식은 데이터를 저장하는데 추가적인 비용이 들어가게 됩니다.
    - 오랜된 것을 파악하기 위해 각 페이지마다 계수기, 스택을 두어야한다는 문제점 발생
            

3. NUR(Not Used Recently) 새로 들어옴, 미사용 = 0 사용 시 = 1 / 교체 조건 ->  0 발견 

일명 clock 알고리즘 최근사용여부를 0,1로 표시하여 교체하는 방법입니다.
1은 최근에 참조, 0은 참조되지 않음을 의미
    
 시계방향으로 돌면서 0을 찾고 0을 찾는 순간 해당 프로세스를 교체, 해당 부분을 1로 바꾸는 알고리즘입니다.
NUR vs LRU
LRU : 데이터를 사용할 때마다 최근 사용 시간을 갱신
NUR : 사용하지 않은 데이터를 주기적으로 스캔하여 최근 사용 여부를 판단

빈도기반 알고리즘

LFU(Least Frequently Used) 사용 시 빈도 + 1 / 교체 조건 -> 빈도 낮은거부터

가장 참조 횟수가 적은 페이지를 교체(= 많이 사용하지 않은 것을 교체)
LFU 방식은 사용 빈도가 낮은 데이터를 제거하여 캐시 히트율을 높일 수 있습니다.
그러나 LFU 알고리즘은 일부 데이터가 빈번하게 사용되는 경우에는 성능 저하가 발생할 수 있습니다.